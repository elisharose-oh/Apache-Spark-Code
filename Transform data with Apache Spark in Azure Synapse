#the following code loads data from all .csv files in the orders folder into a dataframe named order_details and then displays the first five records
order_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)
display(order_details.limit(5))


#uses the split function to separate the values in the CustomerName column into two new columns named FirstName and LastName
#Then it uses the drop method to delete the original CustomerName column

from pyspark.sql.functions import split, col

# Create the new FirstName and LastName fields
transformed_df = order_details.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Remove the CustomerName field
transformed_df = transformed_df.drop("CustomerName")

display(transformed_df.limit(5))

#The following code example saves the dataFrame into a parquet file in the data lake, replacing any existing file of the same name

transformed_df.write.mode("overwrite").parquet('/transformed_data/orders.parquet')
print ("Transformed data saved!")

#partition the output file

from pyspark.sql.functions import year, col

# Load source data
df = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)

# Add Year column
dated_df = df.withColumn("Year", year(col("OrderDate")))

# Partition by year
dated_df.write.partitionBy("Year").mode("overwrite").parquet("/data")

#In the following example, the following code will pull the sales orders, which were placed in 2020

orders_2020 = spark.read.parquet('/partitioned_data/Year=2020')
display(orders_2020.limit(5))

#The following code example saves a dataframe (loaded from CSV files) as an external table name sales_orders. 
#The files are stored in the /sales_orders_table folder in the data lake.

order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table')

#The following code creates two new derived columns named Year and Month and then creates a new table transformed_orders with the new derived columns added.
# Create derived columns
sql_transform = spark.sql("SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders")

# Save the results
sql_transform.write.partitionBy("Year","Month").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table')

#Because this new table was created in the metastore, you can use SQL to query it directly with the %%sql magic key in the first line to indicate that the SQL syntax will be used as shown in the following script
%%sql

SELECT * FROM transformed_orders
WHERE Year = 2021
    AND Month = 1
    
    
 #When working with external tables, you can use the DROP command to delete the table definitions from the metastore without affecting the files in the data lake.
 %%sql

DROP TABLE transformed_orders;
DROP TABLE sales_orders;


